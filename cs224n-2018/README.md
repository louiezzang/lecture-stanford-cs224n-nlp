[Markdown manual](https://guides.github.com/features/mastering-markdown/)

# CS224n: Natural Language Processing with Deep Learning (2018)

## Links
* Home: http://web.stanford.edu/class/cs224n/index.html
* Syllabus: http://web.stanford.edu/class/cs224n/syllabus.html


## Schedule and Syllabus
Event | Date | Description | Course Materials
------------ | ------------- | ------------- | -------------
Lecture1 | Jan 9 | Introduction to NLP and Deep Learning <br>[[slides]](./lecture1.pdf) | Suggested Readings: <br>1. [Linear Algebra Review](./lecture1/cs229-linalg.pdf) <br>2. [Probability Review](./lecture1/cs229-prob.pdf) <br>3. [Convex Optimization Review](./lecture1/cs229-cvxopt.pdf) <br>4. [More Optimization (SGD) Review](http://cs231n.github.io/optimization-1/)
Lecture2 | Jan 11 | Word Vectors 1 <br>[[slides]](./lecture2.pdf) | Suggested Readings: <br>1. [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) <br>2. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) <br>3. [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)
A1 released | Jan 11 | Assignment #1 released| [Assignment #1](http://web.stanford.edu/class/cs224n/assignment1/index.html) <br>[Written Solutions](./assignment1/assignment1-solution.pdf)
Lecture3 | Jan 16 | Word Vectors 2 <br>[[slides]](./lecture3.pdf) | Suggested Readings: <br>1. [GloVe: Global Vectors for Word Representation](http://nlp.stanford.edu/pubs/glove.pdf) <br>2. [Improving Distributional Similarity with Lessons Learned fromWord Embeddings](http://www.aclweb.org/anthology/Q15-1016) <br>3. [Evaluation methods for unsupervised word embeddings](http://www.aclweb.org/anthology/D15-1036)
Lecture4 | Jan 18 | Neural Networks <br>[[slides]](./lecture4.pdf) | Suggested Readings: <br>1. cs231n notes on [[backprop]](http://cs231n.github.io/optimization-2/) and [[network architectures]](http://cs231n.github.io/neural-networks-1/) <br>2. [Review of differential calculus](http://web.stanford.edu/class/cs224n/readings/review-differential-calculus.pdf) <br>3. [Natural Language Processing (almost) from Scratch](https://arxiv.org/pdf/1103.0398v1.pdf) <br>4. [Learning Representations by Backpropagating Errors](http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)

## Assignment Solutions
[Assignment solutions in Google search](https://www.google.co.id/search?newwindow=1&dcr=0&ei=7RNqWo6sKIuDvQSjm4yYAQ&q=cs224n+assignment+solution&oq=cs224n+assignment+solution&gs_l=psy-ab.3...137131977.137144437.0.137144823.32.25.0.0.0.0.0.0..0.0....0...1c.1.64.psy-ab..32.0.0....0.rK309CWPZVo)

All lecture notes, slides and assignments from CS224n
https://github.com/maxim5/cs224n-winter-2017 

## Etc.
### Understanding of Backpropagation
* http://cs231n.github.io/optimization-2/ 
* http://jaejunyoo.blogspot.com/2017/01/backpropagation.html (https://www.ics.uci.edu/~pjsadows/notes.pdf)
* https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
